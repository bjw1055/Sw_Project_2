# -*- coding: utf-8 -*-
import pandas as pd
import numpy as np
import re
import sys
import json
import os
import mysql.connector
from prophet import Prophet
import chardet

COLUMN_ALIASES = {
    'íŒë§¤ì¼': 'ë‚ ì§œ', 'date': 'ë‚ ì§œ', 'Date': 'ë‚ ì§œ',
    'íŒë§¤ìˆ˜ëŸ‰': 'ìˆ˜ëŸ‰', 'quantity': 'ìˆ˜ëŸ‰',
    'ë§¤ì¶œ': 'ë§¤ì¶œì•¡', 'sales': 'ë§¤ì¶œì•¡',
    'amount': 'ë§¤ì¶œì•¡', 'Amount': 'ë§¤ì¶œì•¡', 'Sales': 'ë§¤ì¶œì•¡'
}

def get_db_connection():
    return mysql.connector.connect(
        host='localhost',
        user='root',
        password='0000',
        database='dashboard_db'
    )

def guess_column(candidates, options):
    for col in candidates:
        for opt in options:
            if opt.lower() in col.lower():
                return col
    return None

def normalize_column_names(df):
    new_columns = []
    seen = set()
    for col in df.columns:
        base_col = col.strip().replace('\ufeff', '')
        mapped = COLUMN_ALIASES.get(base_col, base_col)
        if mapped in seen:
            new_columns.append(base_col)
        else:
            new_columns.append(mapped)
            seen.add(mapped)
    df.columns = new_columns
    return df

def remove_duplicate_columns(df):
    return df.loc[:, ~df.columns.duplicated()]

def decode_maybe(val):
    try:
        if isinstance(val, bytes):
            encoding = chardet.detect(val)['encoding'] or 'utf-8'
            return val.decode(encoding)
        return val
    except Exception:
        return val

def apply_exchange_rate(df):
    for col in df.columns:
        if col.strip().lower() in ["currency", "í†µí™”"]:
            df[col] = df[col].astype(str).str.upper()
            usd_mask = df[col] == "USD"
            if "Exchange Rate" in df.columns:
                df.loc[usd_mask, "ë§¤ì¶œì•¡"] = (
                    pd.to_numeric(df.loc[usd_mask, "ë§¤ì¶œì•¡"], errors="coerce") *
                    pd.to_numeric(df.loc[usd_mask, "Exchange Rate"], errors="coerce")
                )
            else:
                df.loc[usd_mask, "ë§¤ì¶œì•¡"] = pd.to_numeric(df.loc[usd_mask, "ë§¤ì¶œì•¡"], errors="coerce") * 1300
            print(f"âœ… í™˜ìœ¨ ì ìš©: {usd_mask.sum()}ê±´ USD â†’ KRW ë³€í™˜", file=sys.stderr)
            break
    return df

def convert_mixed_date(val):
    try:
        if isinstance(val, (int, float)):
            if val > 30000:
                return pd.to_datetime('1899-12-30') + pd.to_timedelta(val, unit='D')
            else:
                return pd.to_datetime(val, errors='coerce')
        elif isinstance(val, str):
            v = val.strip()
            try:
                num = float(v)
                if num > 30000:
                    return pd.to_datetime('1899-12-30') + pd.to_timedelta(num, unit='D')
                else:
                    return pd.to_datetime(num, errors='coerce')
            except ValueError:
                cleaned = v.replace('.', '-').replace('/', '-')
                return pd.to_datetime(cleaned, errors='coerce')
        else:
            return pd.NaT
    except Exception:
        return pd.NaT

def analyze_combined_dataframe(df, project_id=None):
    print(f"ğŸ“¦ ë¶„ì„ ì‹œì‘ - ì´ ë°ì´í„° í–‰ ìˆ˜: {len(df)}", file=sys.stderr)

    df = normalize_column_names(df)
    df = remove_duplicate_columns(df)
    for col in df.columns:
        df[col] = df[col].apply(decode_maybe)
    df = apply_exchange_rate(df)

    date_col = guess_column(df.columns, ["ë‚ ì§œ", "date", "íŒë§¤ì¼", "Date"])
    amount_col = guess_column(df.columns, ["ë§¤ì¶œ", "ë§¤ì¶œì•¡", "ê¸ˆì•¡", "sales", "amount", "Amount"])

    print("ğŸ§© ì»¬ëŸ¼ëª… ë¦¬ìŠ¤íŠ¸:", df.columns.tolist(), file=sys.stderr)
    print("ğŸ§© date_col:", date_col, "/ amount_col:", amount_col, file=sys.stderr)

    for col in df.columns:
        if col != date_col and col.lower() in ["íŒë§¤ì¼", "date", "ë‚ ì§œ"]:
            df.drop(columns=[col], inplace=True)

    df['ë‚ ì§œ'] = df[date_col].apply(convert_mixed_date)
    df[amount_col] = df[amount_col].astype(str).str.replace(",", "").str.strip()
    df['ë§¤ì¶œì•¡'] = pd.to_numeric(df[amount_col], errors='coerce')

    print("ğŸ“ ë‚ ì§œ NaN:", df['ë‚ ì§œ'].isna().sum(), file=sys.stderr)
    print("ğŸ“ ë§¤ì¶œì•¡ NaN:", df['ë§¤ì¶œì•¡'].isna().sum(), file=sys.stderr)
    print("ğŸ“ ìœ íš¨í•œ í–‰ ìˆ˜:", df.dropna(subset=['ë‚ ì§œ', 'ë§¤ì¶œì•¡']).shape[0], file=sys.stderr)

    df = df.dropna(subset=['ë‚ ì§œ', 'ë§¤ì¶œì•¡'])
    if df.empty or len(df) < 10:
        raise ValueError("ìœ íš¨í•œ ë¶„ì„ ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤.")

    df_prophet = df[['ë‚ ì§œ', 'ë§¤ì¶œì•¡']].rename(columns={'ë‚ ì§œ': 'ds', 'ë§¤ì¶œì•¡': 'y'})
    df_prophet = df_prophet.groupby('ds', as_index=False).agg({'y': 'sum'}).sort_values('ds')
    df_prophet = df_prophet[(df_prophet['y'] >= 0) & (df_prophet['y'] <= 1e9)]

    if df_prophet.empty or len(df_prophet) < 10:
        raise ValueError("ì˜ˆì¸¡í•  ìˆ˜ ìˆëŠ” ìœ íš¨í•œ ë°ì´í„°ê°€ ë„ˆë¬´ ì ìŠµë‹ˆë‹¤.")

    df_prophet['y_clipped'] = np.clip(df_prophet['y'], 0, 20000000)
    df_prophet['cap'] = 20000000
    df_prophet['floor'] = 0
    df_fit = df_prophet[['ds', 'y_clipped', 'cap', 'floor']].rename(columns={'y_clipped': 'y'})

    model = Prophet(growth='logistic')
    model.fit(df_fit)
    future = model.make_future_dataframe(periods=7)
    future['cap'] = 20000000
    future['floor'] = 0

    forecast = model.predict(future)
    forecast = forecast[forecast['ds'] > df_prophet['ds'].max()]

    forecast_dict = {
        str(row['ds'].date()): max(0, int(round(row['yhat']))) if pd.notna(row['yhat']) else 0
        for _, row in forecast[['ds', 'yhat']].iterrows()
    }

    result = {
        'total_sales': int(df['ë§¤ì¶œì•¡'].sum()),
        'avg_sales': int(df['ë§¤ì¶œì•¡'].mean()),
        'max_sales': int(df['ë§¤ì¶œì•¡'].max()),
        'min_sales': int(df['ë§¤ì¶œì•¡'].min()),
        'forecast_next_7_days': forecast_dict
    }

    if project_id is not None:
        try:
            conn = get_db_connection()
            cursor = conn.cursor()
            for date_str, amount in forecast_dict.items():
                if not date_str:
                    continue
                raw_data = {"name": "ì˜ˆì¸¡ë°ì´í„°", "amount": amount, "date": date_str}
                cursor.execute(
                    "INSERT INTO products (project_id, raw_data) VALUES (%s, %s)",
                    (project_id, json.dumps(raw_data, ensure_ascii=False))
                )
            conn.commit()
            cursor.close()
            conn.close()
        except Exception as e:
            print(f"DB ì €ì¥ ì˜¤ë¥˜: {e}", file=sys.stderr)

    return result

if __name__ == '__main__':
    args = sys.argv[1:]
    project_id = None
    if '--project' in args:
        idx = args.index('--project')
        if idx + 1 < len(args):
            project_id = int(args[idx + 1])
            del args[idx:idx + 2]

    try:
        conn = get_db_connection()
        df = pd.read_sql("SELECT raw_data FROM products WHERE project_id = %s ORDER BY uploaded_at ASC", conn, params=(project_id,))
        conn.close()

        raw_parsed = []
        for _, item in df.iterrows():
            try:
                row = json.loads(item['raw_data'])
                cleaned = {k.strip().replace('\ufeff', ''): decode_maybe(v) for k, v in row.items() if k}
                for bad_key in ['filename', 'index', '_merge']:
                    cleaned.pop(bad_key, None)
                cleaned = {k: v for k, v in cleaned.items() if not re.match(r"^_\\d+$|^Unnamed", k)}
                raw_parsed.append(cleaned)
            except Exception as e:
                print(f"JSON íŒŒì‹± ì‹¤íŒ¨: {e}", file=sys.stderr)

        df_parsed = pd.DataFrame(raw_parsed)
        if 'ë‚ ì§œ' in df_parsed.columns:
            df_parsed['ë‚ ì§œ'] = df_parsed['ë‚ ì§œ'].astype(object)
        result = analyze_combined_dataframe(df_parsed, project_id)
        print(json.dumps(result, ensure_ascii=False), file=sys.stdout)
    except Exception as e:
        print(json.dumps({'error': f'ë¶„ì„ ì‹¤íŒ¨: {str(e)}'}), file=sys.stdout)

